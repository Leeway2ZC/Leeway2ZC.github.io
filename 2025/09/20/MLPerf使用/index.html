<!DOCTYPE html><html lang="cn"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="1EB8XoOl0C"><meta name="google-site-verification" content="K7thEgdLm0UfRWJ5MGdF7sCcjClSzAlxFLPv2Oz5CGM"><title> MLPerf使用 · LeeWay</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="MLPerf使用 - David Lee"><meta name="keywords"><meta name="author" content="David Lee"><link rel="short icon" href="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/favicon-32x32.png"><link rel="stylesheet" href="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/bubuzou.css"><link rel="search" type="application/opensearchdescription+xml" href="https://leeway2zc.top/atom.xml" title="LeeWay"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g==" crossorigin="anonymous" referrerpolicy="no-referrer" /><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="LeeWay" type="application/atom+xml">
</head><body><header><div class="header row"> <a href="/" class="logo-link"><img src="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20250811154253_3.jpg"></a><ul id="nav_list" class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" data-hover="博文" class="nav-list-link">博文</a></li><li class="nav-list-item"><a href="/archives/" target="_self" data-hover="归档" class="nav-list-link">归档</a></li><li class="nav-list-item"><a href="/categories/live/" target="_self" data-hover="生活" class="nav-list-link">生活</a></li><li class="nav-list-item"><a href="/categories/read/" target="_self" data-hover="读书" class="nav-list-link">读书</a></li><li class="nav-list-item"><a href="/about/" target="_self" data-hover="关于" class="nav-list-link">关于</a></li></ul><div class="search"><a id="search_btn" href="#search"></a></div><div id="nav_btn" class="nav-btn"><span></span><span></span><span></span></div></div></header><div class="row scroll-con"><section class="container"><!-- for archive page--><div id="postAr" class="post"><article class="post-block"><h1 class="post-title">MLPerf使用</h1><div class="post-info">2025-09-20<p id="busuanzi_container_page_pv" class="visit"><span id="busuanzi_value_page_pv"> </span><span>次访问</span></p></div><div class="post-content"><h1><center>MLPerf 学习</center></h1>

<blockquote>
<p>MLPerf 是一个业界标准的机器学习性能基准套件（benchmark suite）</p>
</blockquote>
<blockquote>
<p>它的目标是让不同硬件平台（CPU&#x2F;GPU&#x2F;TPU&#x2F;ASIC）、框架（TensorFlow、PyTorch 等）和系统在 <strong>统一的工作负载</strong> 下进行公平的对比，从而衡量不同硬件&#x2F;系统在标准ML任务上的性能</p>
</blockquote>
<h2 id="一-MLPerf-概念"><a href="#一-MLPerf-概念" class="headerlink" title="一  MLPerf 概念"></a>一  MLPerf 概念</h2><ul>
<li><p>定位：类似于CPU中的SPEC，存储中的fio，MLPerf是机器学习中的基准测试。</p>
</li>
<li><p>作用：旨在对硬件、软件和服务的训练和推理性能进行无偏评估。</p>
</li>
<li><p>MLPerf 主要解决的问题：</p>
<ol>
<li><p>硬件对比</p>
<ul>
<li><p>比如：NVIDIA A100 vs H100、Intel CPU vs AMD CPU、Google TPU vs Habana Gaudi</p>
</li>
<li><p>大家都跑同样的模型（ResNet-50、BERT、DLRM 等），就能公平比较性能。</p>
</li>
</ul>
</li>
<li><p>系统优化对比</p>
<ul>
<li><p>不只是 GPU&#x2F;TPU，还包括 <strong>存储、网络、分布式训练框架、编译优化器</strong> 等。</p>
</li>
<li><p>比如 NCCL vs OneCCL，PyTorch vs TensorFlow，InfiniBand vs Ethernet。</p>
</li>
</ul>
</li>
<li><p>统一标准</p>
<ul>
<li><p>和 SPEC（CPU）、TPC（数据库性能）类似，MLPerf 是 ML 领域的统一标尺。</p>
</li>
<li><p>结果要符合精度门槛（accuracy target）、固定数据集和 batch size，保证公平。</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>基准测试类别</p>
<ul>
<li>MLPerf Training：Training基准套件测量系统训练模型达到目标质量指标的速度。<ul>
<li>MLPerf Training HPC</li>
</ul>
</li>
<li><strong>MLPerf Inference</strong>：Inference基准套件测量系统处理输入和使用训练模型生成结果的速度。<ul>
<li>MLPerf Inference Datacenter</li>
<li>MLPerf Inference Edge</li>
<li>MLPerf Inference Mobile</li>
<li>MLPerf Inference Tiny</li>
</ul>
</li>
<li><strong>MLPerf Storage</strong>：MLPerf存储基准测试套件测量存储系统在训练模型时提供训练数据的速度。</li>
<li>MLPerf Client：用于评估大型语言模型（LLMs）和其他 AI 工作负载在个人计算机上的性能——从笔记本电脑和台式机到工作站。通过模拟真实世界的 AI 任务，它提供了清晰的指标，以便了解系统处理生成性 AI 工作负载的能力。</li>
<li>MLPerf Automotive：用于测量旨在用于汽车的计算机性能，包括高级驾驶辅助系统&#x2F;自动驾驶（ADAS&#x2F;AD）和车载信息娱乐（IVI）嵌入式系统。主要的关键绩效指标是延迟，因为汽车系统是实时的，且通常需要功能安全。</li>
<li>AlgoPerf Training Algorithms：算法基准测试衡量通过更改基础训练算法（例如优化器或超参数）来提高神经网络模型训练速度，以达到给定的目标性能。</li>
<li>AlLuminate：AILuminate基准评估一般聊天机器人生成AI系统的安全性，以帮助指导开发，告知购买者和消费者，并支持标准机构和政策制定者。</li>
</ul>
</li>
<li><p>测试结果基础概念：</p>
<ul>
<li><p>Queries(查询)：指的是一个 推理请求，即向模型提交一个输入数据（例如一张图片、一段文本）并获取模型输出的过程。查询（Query） &#x3D; 输入数据 + 推理任务。</p>
<blockquote>
<p>每个查询通常包括</p>
</blockquote>
<ul>
<li>输入数据：例如一张图片、一段音频、一段文本</li>
<li>模型推理：将输入数据传递给模型，运行推理</li>
<li>输出结果：模型生成的预测结果（例如分类标签、生成文本）</li>
</ul>
</li>
<li><p>QPS(Queries Per Second)：系统每秒能够处理多少个查询（推理请求）</p>
<blockquote>
<p>QPS 描述的是 推理应用 的性能指标，而不是模型本身的指标。它衡量的是整个推理系统的吞吐量</p>
</blockquote>
<ul>
<li>如果 QPS &#x3D; 100，表示系统每秒可以处理 100 个推理请求</li>
<li>如果 QPS &#x3D; 50，表示系统每秒可以处理 50 个推理请求。</li>
<li>这个值越高越好，表明系统能够在单位时间内处理更多请求</li>
</ul>
</li>
<li><p>Mean(Mean Latency，平均延迟)：表示系统处理一个任务（查询&#x2F;推理请求）平均需要多长时间</p>
<ul>
<li>MEAN 越低，说明系统处理任务的速度越快。</li>
</ul>
</li>
</ul>
</li>
<li><p>测试场景类别：</p>
<ul>
<li><p>SingleStream(单流)：模拟单个用户推理请求的场景</p>
<blockquote>
<p>每次处理 一个输入数据（例如一张图片、一段文本）， 且 输入数据是逐张&#x2F;逐条处理的。<br>举个例子，有100张图片，一个图片执行一次查询，每个图片逐张执行一次查询，总共执行100次。</p>
</blockquote>
<ul>
<li>测试目标： 评估系统处理 单个任务（&#x2F;查询） 的速度（延迟）</li>
<li>关键指标：延迟（Latency）</li>
<li>适用场景：对延迟敏感的应用，例如，实时图像分类，实时语音识别，自动驾驶中的实时目标检测</li>
</ul>
</li>
<li><p>MultiStream(多流)：模拟多个用户同时请求的场景</p>
<blockquote>
<p>每个用户请求都是独立执行的。相当于多个 SingleStream 加在一起，但多个请求会同时并发执行<br>类似于OS单个进程和多个进程</p>
</blockquote>
<ul>
<li>测试目标： 评估系统在同时处理多个并发请求 时的性能</li>
<li>关键指标： 吞吐量（Throughput）&amp; 延迟（Latency）</li>
<li>适用场景：对吞吐量和延迟都有要求的应用，例如，多用户语音助手，多摄像头监控系统</li>
</ul>
</li>
<li><p>Offline(离线)：模拟单用户批量处理的场景</p>
<blockquote>
<p>系统一次性处理大量输入数据（例如一批图片、一段长文本）来执行一次查询（推理请求）<br>由于模型一般地都支持一次性输入多个数据来执行一次查询，所以离线模式研究多个输入下模型的处理能力，这个瓶颈就在QPS<br>举个例子，有100张图片，给模型一次输入20张，然后模型一次推理后，输出20张的结果，然后进入下一个批处理周期，直到处理完成</p>
</blockquote>
<ul>
<li>测试目标：评估系统处理 大量任务 的能力（吞吐量）</li>
<li>关键指标： 吞吐量（Throughput）</li>
<li>适用场景：对吞吐量要求高、对延迟不敏感的应用，例如，批量图像处理，大规模文本生成，离线数据分析</li>
</ul>
</li>
<li><p>Server(服务器)：模拟服务器端推理的场景</p>
<blockquote>
<p>系统需要同时处理多个并发请求，并在规定时间内返回结果</p>
</blockquote>
<ul>
<li>关键指标： 吞吐量（Throughput）&amp; 延迟（Latency）</li>
<li>适用场景：服务器端推理应用，例如，云端的图像分类服务，在线的语音识别服务，实时推荐系统</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="二-MLPerf-使用流程"><a href="#二-MLPerf-使用流程" class="headerlink" title="二  MLPerf 使用流程"></a>二  MLPerf 使用流程</h2><h3 id="1-MLPerf-Inference"><a href="#1-MLPerf-Inference" class="headerlink" title="1 MLPerf Inference"></a>1 MLPerf Inference</h3><blockquote>
<p>测试计算机视觉中图像分类和检测的性能</p>
</blockquote>
<p><strong>测试环境：</strong></p>
<ol>
<li>机器：Google Colab</li>
<li>基准模型：mobilenet、resnet50</li>
<li>模型框架：onnx</li>
<li>场景：视觉vision（图像分类）</li>
<li>数据集：fakeimagent</li>
</ol>
<p><strong>测试步骤：</strong></p>
<ol>
<li><p>克隆MLPerf Inference项目库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/mlperf/inference.git</span><br><span class="line"><span class="built_in">cd</span> inference/vision/classification_and_detection</span><br><span class="line">import os</span><br><span class="line">root = os.getcwd()</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装必要的依赖库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apt-get install python3-dev</span><br><span class="line">apt-get install cmake</span><br><span class="line">pip install  pytest</span><br><span class="line">pip install  numpy</span><br><span class="line">pip install  scipy</span><br><span class="line">pip install  pybind11</span><br><span class="line">pip install onnxruntime pycocotools opencv-python</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行安装脚本</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ../../loadgen; CFLAGS=<span class="string">&quot;-std=c++14&quot;</span> python setup.py develop; <span class="built_in">cd</span> &#123;root&#125;</span><br><span class="line">python setup.py develop</span><br></pre></td></tr></table></figure>

<p>安装成功应该在最后会显示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Using /usr/local/lib/python3.12/dist-packages</span><br><span class="line">Finished processing dependencies for mlperf-inference==0.1.0</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载模型</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载 mobilenet 模型</span></span><br><span class="line">wget -q https://zenodo.org/record/3157894/files/mobilenet_v1_1.0_224.onnx</span><br><span class="line"><span class="comment"># mobilenet 为 resnet50 模型的轻量级模型，以降低模型复杂性和计算负担</span></span><br><span class="line"><span class="comment"># 下载 resnet50 模型</span></span><br><span class="line">wget -q https://zenodo.org/record/2592612/files/resnet50_v1.onnx</span><br></pre></td></tr></table></figure>

<blockquote>
<p>其他模块链接地址<a target="_blank" rel="noopener" href="https://github.com/mlcommons/inference/tree/master/vision/classification_and_detection">https://github.com/mlcommons/inference/tree/master/vision/classification_and_detection</a></p>
</blockquote>
</li>
<li><p>下载数据集</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在本测试中，使用 MLPerf 提供的 tools/make_fake_imagenet.sh 工具创建一个假装为 imagenet 的小型假数据集</span></span><br><span class="line">bash ./tools/make_fake_imagenet.sh</span><br><span class="line"><span class="comment"># 运行上述命令后，将在 /vision/classification_and_detection 文件夹中创建一个 fakeimagenet 文件夹，里面包含 val 文件夹和 val_map.txt 文件，val 文件夹中存放着 8 张假的图像数据集</span></span><br><span class="line"><span class="comment"># 通常需要下载 imagenet2012/valiation 进行图像分类，或下载 coco2017/valiation 进行对象检测</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>其他数据集的链接和说明地址<a target="_blank" rel="noopener" href="https://github.com/mlcommons/inference/tree/master/vision/classification_and_detection">https://github.com/mlcommons/inference/tree/master/vision/classification_and_detection</a></p>
</blockquote>
</li>
<li><p>添加环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">os.environ[&#x27;MODEL_DIR&#x27;] = root</span><br><span class="line">os.environ[&#x27;DATA_DIR&#x27;] = os.path.join(root, &quot;fake_imagenet&quot;)</span><br></pre></td></tr></table></figure>

<p>对于 mlperf 提交的查询数、时间、延迟和百分位数，一般默认使用的设置。这个测试收中参考了官方教程，传递了一些额外的选项来让测试进展得更快。 run_local.sh 将查找环境变量 EXTRA_OPS 并将其添加到参数中。 还可以在命令行中添加其他参数。 以下选项将基准测试的运行时间限制为 10 秒，并添加准确性报告。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.environ[&#x27;EXTRA_OPS&#x27;] =&quot;--time 10 --max-latency 0.2&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>把 <code>remove_initializer_from_input.py</code> 文件放到 <code>classification_and_detection</code> 文件夹</p>
</li>
</ol>
<blockquote>
<p>文件链接地址为<a target="_blank" rel="noopener" href="https://github.com/microsoft/onnxruntime/blob/main/tools/python/remove_initializer_from_input.py">https://github.com/microsoft/onnxruntime/blob/main/tools/python/remove_initializer_from_input.py</a></p>
</blockquote>
<ol start="8">
<li><p>执行以下命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python remove_initializer_from_input.py --input ./mobilenet_v1_1.0_224.onnx --output ./mobilenet_v1_1.0_224.onnx</span><br></pre></td></tr></table></figure>
</li>
<li><p>再运行基准测试(mobilenet模型)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./run_local.sh onnxruntime mobilenet cpu --scenario SingleStream</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>--scenario &#123;SingleStream,MultiStream,Server,Offline&#125;</code> 选择要测试基准的场景</p>
</blockquote>
</li>
</ol>
<p><img src="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/image-20250919091351616.png" alt="image-20250919091351616"></p>
<ol start="8">
<li><p>输出结果保存在 output 文件夹中</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> ./output/onnxruntime-cpu/mobilenet</span><br></pre></td></tr></table></figure></li>
</ol>
<img src="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/image-20250919091526148.png" alt="image-20250919091526148" style="zoom:50%;" />

<ol start="8">
<li><p>测试结果存储在 <code>mlperf_log_summary.txt</code> 文件中，可查看该日志文件中的内容获取测试结果</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> ./output/onnxruntime-cpu/mobilenet/mlperf_log_summary.txt</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>总结：</strong></p>
<ul>
<li><p>主要步骤：</p>
<ol>
<li>安装MLPerf inference库和必要的依赖</li>
<li>下载模型</li>
<li>下载数据集</li>
<li>执行基准测试</li>
</ol>
</li>
<li><p>基准测试应用程序使用 shell 脚本来简化命令行选项，用户可以选择后端、模型和设备</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">!./run_local.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果：</span></span><br><span class="line"><span class="string">&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">usage: ./run_local.sh [tf|onnxruntime|pytorch|tflite|tvm-onnx|tvm-pytorch|tvm-tflite] [resnet50|mobilenet|ssd-mobilenet|ssd-resnet34|retinanet] [cpu|gpu] --scenario [SingleStream|MultiStream|Offline|Server]</span></span><br><span class="line"><span class="string">&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">后端是 [tf|onnxruntime|pytorch|tflite|tvm-onnx|tvm-pytorch] 之一</span><br><span class="line">模型是 [resnet50|retinanet|mobilenet|ssd-mobilenet|ssd-resnet34] 之一</span><br><span class="line">设备是 [cpu|gpu] 之一</span><br><span class="line">场景是 [SingleStream|MultiStream|Offline|Server] 之一</span><br></pre></td></tr></table></figure>

<p><strong>测试结果：</strong></p>
<ol>
<li>基本信息</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SUT name : PySUT</span><br><span class="line">Scenario : SingleStream</span><br><span class="line">Mode     : PerformanceOnly</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>SUT name</strong>：System Under Test（被测系统）的名字，这里叫 <code>PySUT</code></li>
<li><strong>Scenario</strong>：运行场景，这里是 <strong>SingleStream</strong>（模拟一条推理请求流）</li>
<li><strong>Mode</strong>：运行模式，这里是 <strong>PerformanceOnly</strong>，说明只测性能，不测精度</li>
</ul>
<hr>
<ol start="2">
<li>结果摘要</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">90.0th percentile latency (ns) : 20354752</span><br><span class="line">Result is : VALID</span><br><span class="line">  Min duration satisfied : Yes</span><br><span class="line">  Min queries satisfied : Yes</span><br><span class="line">  Early stopping satisfied: Yes</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>90.0th percentile latency (ns)</strong>：90 分位延迟，也就是 90% 的请求在 20,354,752 纳秒（约 20.3 ms）以内完成</li>
<li><strong>Result is : VALID</strong>：说明结果合规</li>
<li><strong>Min duration satisfied</strong>：测试时间达到最低要求</li>
<li><strong>Min queries satisfied</strong>：推理请求数量达到最低要求</li>
<li><strong>Early stopping satisfied</strong>：启用了早停机制，判定结果有效</li>
</ul>
<hr>
<ol start="3">
<li>Early Stopping（早停机制）</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Early Stopping Result:</span><br><span class="line"> * Processed at least 64 queries (659).</span><br><span class="line"> * Would discard 47 highest latency queries.</span><br><span class="line"> * Early stopping 90.0th percentile estimate: 20960121</span><br><span class="line"> * Not enough queries processed for 99.0th percentile</span><br><span class="line">   early stopping estimate (would need to process at</span><br><span class="line">   least 662 total queries).</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Processed at least 64 queries (659)</strong>：实际处理了 659 次推理请求，超过最低 64 次的要求</li>
<li><strong>Would discard 47 highest latency queries</strong>：早停算法会丢掉最慢的 47 个请求，再估计分位数</li>
<li><strong>Early stopping 90.0th percentile estimate</strong>：估算的 90% 分位延迟是 20,960,121 ns（≈20.96 ms）</li>
<li><strong>Not enough queries for 99%</strong>：没有足够请求数（需要 ≥662），所以没法估算 99% 分位</li>
</ul>
<hr>
<ol start="4">
<li>性能统计</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">QPS w/ loadgen overhead         : 65.75</span><br><span class="line">QPS w/o loadgen overhead        : 65.82</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>QPS w&#x2F; loadgen overhead</strong>：带上 MLPerf LoadGen（负载生成器）的开销后，每秒处理 65.75 个请求</li>
<li><strong>QPS w&#x2F;o loadgen overhead</strong>：纯粹推理请求处理速度，每秒 65.82 个请求</li>
</ul>
<p>二者接近，说明 LoadGen 对性能影响不大。</p>
<hr>
<ol start="5">
<li>延迟分布</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Min latency (ns)                : 11783153   (~11.8 ms)</span><br><span class="line">Max latency (ns)                : 26771548   (~26.8 ms)</span><br><span class="line">Mean latency (ns)               : 15193210   (~15.2 ms)</span><br><span class="line">50.00 percentile latency (ns)   : 13654569   (~13.7 ms)</span><br><span class="line">90.00 percentile latency (ns)   : 20354752   (~20.4 ms)</span><br><span class="line">95.00 percentile latency (ns)   : 21623462   (~21.6 ms)</span><br><span class="line">97.00 percentile latency (ns)   : 22250577   (~22.3 ms)</span><br><span class="line">99.00 percentile latency (ns)   : 23239962   (~23.2 ms)</span><br><span class="line">99.90 percentile latency (ns)   : 26771548   (~26.8 ms)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Min latency</strong>：最快的一次推理 ≈ 11.8 ms</li>
<li><strong>Max latency</strong>：最慢的一次推理 ≈ 26.8 ms</li>
<li><strong>Mean latency</strong>：平均 ≈ 15.2 ms</li>
<li><strong>50% 分位</strong>：一半请求低于 ≈ 13.7 ms</li>
<li><strong>90% 分位</strong>：90% 请求低于 ≈ 20.4 ms（这是主要考核指标）</li>
<li><strong>95%&#x2F;97%&#x2F;99%&#x2F;99.9% 分位</strong>：高百分位延迟，反映尾部性能（越低越好）</li>
</ul>
<hr>
<ol start="6">
<li>总结</li>
</ol>
<ul>
<li>系统 <strong>SingleStream 延迟中位数 ~13.7 ms，90% 分位 ~20.3 ms</strong></li>
<li>吞吐量大概 <strong>65 QPS</strong></li>
<li>结果 <strong>VALID</strong>，性能合格</li>
<li>延迟分布没有太大的长尾，尾延迟（99%）约 23 ms</li>
</ul>
<h3 id="2-MLPerf-Storage"><a href="#2-MLPerf-Storage" class="headerlink" title="2 MLPerf Storage"></a>2 MLPerf Storage</h3><blockquote>
<p>测试存储系统在训练模型时提供训练数据的速度</p>
</blockquote>
<p>测试步骤：</p>
<ol>
<li><p>安装依赖</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt update</span><br><span class="line"><span class="built_in">sudo</span> apt install python3-pip python3-venv libopenmpi-dev openmpi-common</span><br><span class="line">python3.10 -m venv ~/.venvs/mlpstorage</span><br><span class="line"><span class="built_in">source</span> ~/.venvs/mlpstorage/bin/activate</span><br><span class="line">python3 -m pip install --upgrade pip</span><br><span class="line">git <span class="built_in">clone</span> -b v2.0 https://github.com/mlcommons/storage.git</span><br><span class="line"><span class="built_in">cd</span> storage</span><br><span class="line">pip3 install -e . -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算training datasize</p>
<p><img src="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/image-20250919170809174.png" alt="image-20250919170809174"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例如</span></span><br><span class="line">mlpstorage training datasize -m unet3d --client-host-memory-in-gb 128 --max-accelerators 16 --num-client-hosts 2 --accelerator-type a100  --results-dir ~/mlps-results</span><br><span class="line"><span class="comment"># 计算在两台各有128 GB内存的客户端机器上运行的unet3d模型的最小数据集大小，总共模拟了8个A100加速器</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>生成 training data </p>
<p><img src="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/image-20250919170859168.png" alt="image-20250919170859168"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例如，其中IP1和IP2是两个节点，一个是主机节点一个是客户端节点，在主机上执行命令即可</span></span><br><span class="line">mlpstorage training datagen --hosts IP1,IP2 --model unet3d --num-processes 8 --data-dir /mnt/unet3d_data --param dataset.num_files_train=56000</span><br><span class="line"><span class="comment"># 生成56,000个unet3d工作负载的训练数据，将其放入unet3d_data目录，使用8个并行作业分布在2个节点上</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>运行 Trainning 测试</p>
<p><img src="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/image-20250919171547618.png" alt="image-20250919171547618"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例如</span></span><br><span class="line">mlpstorage training run --hosts 10.117.61.121,10.117.61.165 --num-client-hosts 2 --client-host-memory-in-gb 64 --num-accelerators 2 --accelerator-type h100 --model unet3d  --data-dir unet3d_data --results-dir unet3d_results    --param dataset.num_files_train=400</span><br><span class="line"><span class="comment"># 使用位于 unet3d_data 目录中的数据，对 unet3d 工作负载进行基准测试，使用2个分布在2个客户端主机上的 H100 加速器（IP 地址为 10.117.61.121 和 10.117.61.165），并将结果保存到 unet3d_results 目录中</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看测试结果</p>
<p><img src="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/image-20250919171720452.png" alt="image-20250919171720452"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例如，这个脚本必须在主机节点(launcher client host)上运行</span></span><br><span class="line">mlpstorage reports reportgen --out-dir ./results</span><br><span class="line"><span class="comment"># 将结果保存在results目录下</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p>单机测试：使用U-Net3D模型，至少需要1张A100或H100</p>
<ol>
<li><p>计算数据大小</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mlpstorage training datasize --model unet3d --client-host-memory-in-gb 32 --num-client-hosts 1 --max-accelerators 1 --accelerator-type h100</span><br><span class="line"><span class="comment"># --client-host-memory-in-gb 是指主机可用内存大小</span></span><br><span class="line"><span class="comment"># --num-client-hosts 是指节点数量，因为是单机测试所以设置为1</span></span><br><span class="line"><span class="comment"># --max-accelerators 是指可用的计算卡数量</span></span><br><span class="line"><span class="comment"># --accelerator-type 可选a100或h100</span></span><br></pre></td></tr></table></figure>

<p><img src="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/image-20250920104100306.png" alt="image-20250920104100306"></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 结果说明</span></span><br><span class="line"><span class="comment">// 1.文件数量要求</span></span><br><span class="line">RESULT: Minimum file count dictated by <span class="number">500</span> step requirement of given accelerator count and batch size.</span><br><span class="line">RESULT: Number of training files: <span class="number">3500</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">500 step requirement: MLPerf规定训练必须至少运行500个步骤（steps）</span></span><br><span class="line"><span class="comment">3500个文件: 基于1张A100 GPU的批处理大小和500步要求计算出的最小文件数</span></span><br><span class="line"><span class="comment">这确保有足够的数据让训练跑满500个iteration</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// 2.存储需求</span></span><br><span class="line">RESULT: Total disk space required <span class="keyword">for</span> training: <span class="number">477.86</span> GB</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">总共需要约478 GB的磁盘空间</span></span><br><span class="line"><span class="comment">平均每个文件大小：478 GB ÷ 3500 ≈ 140 MB</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// 3.数据组织</span></span><br><span class="line">RESULT: Number of training subfolders: <span class="number">0</span></span><br><span class="line"><span class="comment">/* 所有文件将放在同一个目录下，不使用子文件夹结构 */</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>生成训练数据</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mlpstorage training datagen --hosts 127.0.0.1 --num-processes 8 --model unet3d --data-dir unet3d_data --results-dir unet3d_results  --param dataset.num_files_train=500</span><br><span class="line"><span class="comment"># --num-processes 是指使用的处理器数量，越多生成的越快，用lscpu看有多少个cpu数量</span></span><br><span class="line"><span class="comment"># dataset.num_files_train 是指训练文件数量</span></span><br></pre></td></tr></table></figure>

<p><img src="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/image-20250920103834291.png" alt="image-20250920103834291"></p>
</li>
<li><p>运行测试</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlpstorage training run --hosts 127.0.0.1 --num-client-hosts 1 --client-host-memory-in-gb 32 --num-accelerators 1 --accelerator-type a100 --model unet3d  --data-dir unet3d_data --results-dir unet3d_results --param dataset.num_files_train=500 --allow-run-as-root</span><br></pre></td></tr></table></figure>

<p><img src="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/image-20250920104942998.png" alt="image-20250920104942998"><img src="https://leeway2zcblog-1373523181.cos.ap-guangzhou.myqcloud.com/img/image-20250920105039197.png" alt="image-20250920105039197"></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 结果说明</span></span><br><span class="line"><span class="comment">// 1. 配置验证结果</span></span><br><span class="line">ERROR: INVALID: [INVALID] Insufficient number of training <span class="title function_">files</span> </span><br><span class="line"><span class="params">(Parameter: dataset.num_files_train, Expected: &gt;= <span class="number">3500</span>, Actual: <span class="number">500</span>)</span></span><br><span class="line">- 状态: 配置无效（INVALID）</span><br><span class="line">- 原因: 文件数量不足，需要至少3500个文件</span><br><span class="line">- 影响: 结果不能用于正式MLPerf提交，但测试依然运行</span><br><span class="line"><span class="comment">// 2. 运行模式</span></span><br><span class="line">WARNING: Running the benchmark without verification <span class="keyword">for</span> open or closed configurations. </span><br><span class="line">These results are not valid <span class="keyword">for</span> submission.</span><br><span class="line">- 在非验证模式下运行</span><br><span class="line">- 结果仅用于性能测试，不符合MLPerf提交标准</span><br><span class="line"></span><br><span class="line"><span class="comment">// 性能指标分析</span></span><br><span class="line"><span class="comment">// 1. 训练配置</span></span><br><span class="line">Max steps per epoch: 71 = <span class="number">1</span> * <span class="number">500</span> / <span class="number">7</span> / <span class="number">1</span> (samples per file * num files / batch size / comm size)</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">- 每个epoch步数: 71步 (500个文件 ÷ 7 batch size = 71步)</span></span><br><span class="line"><span class="comment">- 运行了5个epochs: 总共355步</span></span><br><span class="line"><span class="comment">- 每步时间: 约0.637秒</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// 2. 关键性能指标</span></span><br><span class="line">Training Accelerator Utilization [AU] (%): <span class="number">30.48</span>%</span><br><span class="line">train_au_meet_expectation: fail</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">GPU利用率 (AU - Accelerator Utilization)</span></span><br><span class="line"><span class="comment">- 平均GPU利用率: 30.48%</span></span><br><span class="line"><span class="comment">- 状态: 失败 - GPU利用率过低</span></span><br><span class="line"><span class="comment">- 问题: 存储I/O成为瓶颈，GPU等待数据</span></span><br><span class="line"><span class="comment">吞吐量表现</span></span><br><span class="line"><span class="comment">Training Throughput (samples/second): 3.30</span></span><br><span class="line"><span class="comment">Training I/O Throughput (MB/second): 461.55</span></span><br><span class="line"><span class="comment">- 训练吞吐量: 3.30 样本/秒</span></span><br><span class="line"><span class="comment">- I/O吞吐量: 461.55 MB/秒</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 性能问题分析</span></span><br><span class="line"><span class="comment">// 1. GPU利用率低的原因</span></span><br><span class="line">Epoch <span class="number">1</span><span class="number">-5</span> AU范围: <span class="number">29.81</span>% - <span class="number">31.42</span>%</span><br><span class="line">平均计算时间: <span class="number">0.6368</span>±<span class="number">0.0034</span>秒/步</span><br><span class="line">- 存储I/O瓶颈: GPU经常等待数据加载</span><br><span class="line">- 数据管道效率低: 数据预处理跟不上GPU消耗速度</span><br><span class="line"><span class="comment">// 2. 系统资源警告</span></span><br><span class="line">[WARNING] Running DLIO with <span class="number">4</span> threads <span class="keyword">for</span> I/O but core available <span class="number">2</span> are insufficient </span><br><span class="line">and can lead to lower performance.</span><br><span class="line">- CPU核心不足: 只有<span class="number">2</span>个可用核心，但需要<span class="number">4</span>个I/O线程</span><br><span class="line">- 建议: 增加CPU资源或减少I/O线程数</span><br><span class="line"><span class="comment">// 3. 数据集问题</span></span><br><span class="line">[WARNING] Number of files <span class="keyword">for</span> training in unet3d_data/unet3d/train (<span class="number">830</span>) is more than <span class="title function_">requested</span> <span class="params">(<span class="number">500</span>)</span>. </span><br><span class="line">A subset of files will be used</span><br><span class="line">- 实际生成了830个文件，但只使用了500个</span><br><span class="line">- 建议清理多余文件或使用完整数据集</span><br><span class="line"></span><br><span class="line"><span class="comment">//检查点和模型信息</span></span><br><span class="line">Model size: 0.000010 GB</span><br><span class="line">Total checkpoint size: 0.000010 GB</span><br><span class="line">Checkpoint save time: 0.0059 s</span><br><span class="line">Checkpoint throughput: 0.0016 GB/s</span><br><span class="line">- 模型很小（仅10KB），主要瓶颈在数据I/O</span><br><span class="line">- 检查点保存速度正常</span><br><span class="line"></span><br><span class="line"><span class="comment">// 测试结果状态</span></span><br><span class="line">- ✅ 功能性成功: 训练流程正常运行</span><br><span class="line">- ✅ 数据加载正常: 数据读取和训练循环工作正常  </span><br><span class="line">- ❌ 性能不达标: GPU利用率过低 <span class="params">(<span class="number">30</span>% vs 期望的&gt;<span class="number">80</span>%)</span></span><br><span class="line">- ❌ 配置不合规: 文件数量不符合MLPerf规范</span><br><span class="line">- ❌ 资源不足: CPU资源不足影响I/O性能</span><br></pre></td></tr></table></figure></li>
</ol>
</div></article></div><div class="right-container"><div class="widget"><div id="arAnchorBar"></div></div></div></section></div><div class="right-menu"></div><div class="modal search-modal"><div class="input-field"><input type="text" id="search_input"><label for="search-input">搜索</label></div><div id="search_result" class="search-result"></div></div><div class="blog-overlay"></div><footer class="row"><div class="footer-con"><div class="paginator"><a href="/2025/09/26/%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83/" title="总结与思考" class="prev">PREV</a><a href="/2025/09/17/VScode%E8%BF%9E%E6%8E%A5%E8%99%9A%E6%8B%9F%E6%9C%BA/" title="VScode连接虚拟机" class="next">NEXT</a></div><div class="copyright"><p>© 2016 - 2025 <a target="_blank">David Lee</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <br> and <a href="https://github.com/Bulandent/hexo-theme-bubuzou" target="_blank">hexo-theme-bubuzou</a></p><p><span id="busuanzi_container_site_pv">本站总访问数：<span id="busuanzi_value_site_pv"></span></span><span id="busuanzi_container_site_uv" style="padding-left: 6px;">访客数：<span id="busuanzi_value_site_uv"></span></span></p><p> <span style="padding-right: 6px;">闽ICP备16007301号-2</span></p></div><div class="totop"><i></i></div></div></footer><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/jquery-1.8.2.min.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/articleCatalog.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/main.js"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script><script>const valineAPI = (() => {
try {
    AV.init("aD8jJBpu4oew3ovNY73z6Rdq-gzGzoHsz", "FdzS5SOPHdhYQoEUngQ8K2QW");
} catch(error) {}
const isExist = (identity) => {
    identity = identity || getRealPath();
    let query = new AV.Query('Timer');
    return new Promise((resolve, reject) => {
    query.equalTo("identity", identity);
    query.find().then(results => {
        resolve(results.length > 0);
    }, error => reject(error));
    })
}

const _get = (identity) => {
    let query = null;
    if(identity && identity instanceof Array){
    let querys = [];
    for(let i = 0; i < identity.length; ++i) {
        querys[i] = new AV.Query('Timer');
        querys[i].equalTo('identity', identity[i]);
    }
    query = AV.Query.or.apply(null ,querys);
    } else {
    identity = identity || getRealPath();
    query = new AV.Query("Timer");
    query.equalTo("identity", identity);
    }

    return new Promise((resolve, reject) => {
    query.find()
    .then(results => resolve(results))
    .catch(error => reject(error))
    })
}

const create = (identity) => {
    identity = identity || getRealPath();
    return new Promise((resolve, reject) => {
    let Todo = AV.Object.extend('Timer');
    let todo = new Todo();
    todo.set("times", 1);
    todo.set("identity", identity);
    todo.save().then(res => resolve(true), error => reject(error));
    })
}

const update = (identity) => {
    identity = identity || getRealPath();
    return new Promise((resolve, reject) => {
    let query = new AV.Query('Timer');
    query.equalTo("identity", identity);
    query.find().then(todos => {
        todos.forEach(todo => {
        todo.set("times", todo.attributes.times + 1);
        });
        return AV.Object.saveAll(todos);
    }).then(todos => resolve(true), error => reject(error));
    })
}

return {
    isExist,
    _get,
    update,
    create
}
})()

const calcAndWriteTimes = () => {
let isPost = true;

let timerAllDOM = document.querySelectorAll(".article-timer");

if(isPost) {
    let identity = timerAllDOM[0].getAttribute("data-identity");
    valineAPI.isExist(identity)
    .then(exist => {
    if(exist) {
        return valineAPI.update(identity);
    }
    return new Promise(resolve => resolve(true));
    })
    .then( succuess => valineAPI._get(identity))
    .then( result => timerAllDOM[0].innerText = result[0].attributes.times)
    .catch(error => console.log(error.message))
    return ;
}

let timerDOMCache = {};

for(let timerDOM of timerAllDOM) {
    let identity = timerDOM.getAttribute("data-identity");
    if(timerDOMCache.hasOwnProperty(identity)){
    timerDOMCache[identity].dom.push(timerDOM);
    }else{
    timerDOMCache[identity] = {
        dom: [timerDOM],
        times: undefined
    };
    }
}

let identities = Object.keys(timerDOMCache);
valineAPI._get(identities).then(results => {
    for(let result of results) {
    let {identity, times} = result.attributes;
    timerDOMCache[identity].times = times;
    timerDOMCache[identity].dom.map(item => item.innerText = times);
    }
    for(let identity of identities) {
    if(timerDOMCache[identity].times){
        continue;
    }
    timerDOMCache[identity].dom.map(item => item.innerText = 1);
    valineAPI.create(identity);
    }
}).catch(error => console.log(error.message))
}

if(true){
calcAndWriteTimes();
}</script></body></html>